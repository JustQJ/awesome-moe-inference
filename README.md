# Awesome MoE LLM Inference System and Algorithm
![Awesome](https://awesome.re/badge.svg)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/JustQJ/awesome-moe-inference/pulls)

A curated list of awesome papers about optimizing the inference of MoE-based LLMs.

Example: [Conference'year] [Paper Title]() [[Code]()]

## Contents


## Survey
[Arxiv'24] [A Survey on Mixture of Experts](https://arxiv.org/abs/2407.06204)

[Arxiv'22] [A Review of Sparse Expert Models in Deep Learning](https://arxiv.org/abs/2209.01667)

## SOTA MoE LLMs




## Model-Level Optimizations

### Efficient Architecture Design

#### Attention Module

#### MoE Module


### Model Compression

#### Pruning
#### Quantization
#### Knowledge Distillation
#### Low Rank Decomposition

## Algorithm-Level Optimization

### Expert Skip
### Speculative Decoding


## System-Level Optimization

### Expert Parallel

### Expert Offloading

[Arxiv'24] [Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models](https://arxiv.org/abs/2402.07033) [[Code](https://github.com/efeslab/fiddler)]




### Load balancing/scheduling

## Contribute

